{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking unsupervised and supervised models on the 4 site combinatorial library of TrpB\n",
    "1. Johnston, K. E. et al. A combinatorially complete epistatic fitness landscape in an enzyme active site. Proceedings of the National Academy of Sciences 121, e2400439121 (2024).\n",
    "\n",
    "\n",
    "The work in this notebook will look fundamentally similar to a subset of the work done by [_Hsu et al._ ](https://www.nature.com/articles/s41587-021-01146-5), but in a single, readable, notebook. Additionally, we consider more than just OHE for AA embeddings. \n",
    "\n",
    "We will probe slightly different factors than the original work as an \"update\" for those conclusions, on a epistatic benchmark. The goal is to do so in a way that highlights the integrated nature of AIDE and showcase how many different model types/methods can be accessed ina  single, readable, notebook with the API in place.\n",
    "\n",
    "Differences in this work:\n",
    "- Compare zero shot methods not in original paper to EV mutation: ESM2 and MSA transformer wild type marginal\n",
    "- Compare different embedding methods as oposed to just one hot encoding: ESM2 mean pooling over whole sequence, ESM2 mean pooling over only the 4 variable residues\n",
    "- Compare linear to a nonlinear top model\n",
    "- Conduct 5 fold CV not just for hyperparameter optimization, but also test set performance. Repeat with 20 random instantiations like the original paper.\n",
    "\n",
    "Naming conventions:\n",
    "\n",
    "`<zs_model>_<embedding>_<top_model>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from scipy.stats import loguniform, uniform\n",
    "\n",
    "import aide_predict as ap\n",
    "from aide_predict.utils.msa import MSAProcessing\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC\n",
    "\n",
    "0. [Acquiring the data](#0-acquiring-the-data)\n",
    "1. [Data preprocessing](#1-data-preprocessing)\n",
    "2. [Self supervised predictors](#2-self-supervised-predictors)\n",
    "3. [Supervised learning](#3-supervised-learning)\n",
    "4. [Combined models](#4-combined-models)\n",
    "5. [Compare results](#5-compare-and-visualize-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Acquiring the data\n",
    "1. Download and extract the data from [here](https://data.caltech.edu/records/h5rah-5z170). You want `data.zip`\n",
    "2. Extract the data to a directory of your choice\n",
    "3. Assign the global variable `RAW_DATA_DIR` to the extracted directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = os.path.join('.', 'data', 'epistatic', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Retrieve the assay labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(RAW_DATA_DIR, 'figure_data', '4-site_merged_replicates', '20230827', 'four-site_summary_AA_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['AAs', 'avg_mu-bg/max']].rename(columns={'AAs': 'sequence', 'avg_mu-bg/max': 'fitness'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove star aas\n",
    "df = df[~df['sequence'].apply(lambda x: '*' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Define wildtype sequence, get full sequences of variants\n",
    "\n",
    "The wildtype in the study is not acutally the wildtype protein, they started with a variant. Define the true wt, apply mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = ap.ProteinSequence(\n",
    "    'MKGYFGPYGGQYVPEILMGALEELEAAYEGIMKDESFWKEFNDLLRDYAGRPTPLYFARRLSEKYGARVYLKREDLLHTGAHKINNAIGQVLLAKLMGKTRIIAETGAGQHGVATATAAALFGMECVIYMGEEDTIRQKLNVERMKLLGAKVVPVKSGSRTLKDAIDEALRDWITNLQTTYYVFGSVVGPHPYPIIVRNFQKVIGEETKKQIPEKEGRLPDYIVACVSGGSNAAGIFYPFIDSGVKLIGVEAGGEGLETGKHAASLLKGKIGYLHGSKTFVLQDDWGQVQVSHSVSAGLDYSGVGPEHAYWRETGKVLYDAVTDEEALDAFIELSRLEGIIPALESSHALAYLKKINIKGKVVVVNLSGRGDKDLESVLNHPYVRERIRLEHHHHHH',\n",
    "    id='wt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations = ['P18G', 'E29G', 'I68V', 'K95L', 'P139L', 'N166D', 'I183F', 'L212P', 'G227S', 'T291S']\n",
    "wt = wt.mutate(mutations, one_indexed=False)\n",
    "wt.id='Tm8D9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = ap.ProteinSequences.from_fasta(os.path.join(RAW_DATA_DIR, 'EVMutation', 'TARGET_b0.1', 'align', 'TARGET_b0.1.a2m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt.id = msa[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_prc = MSAProcessing()\n",
    "msa_sub = msa_prc.process(msa, focus_seq_id=wt.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = np.random.choice(len(msa_sub), replace=False, size=1000, p=msa_sub.weights/sum(msa_sub.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = [0] + list(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_sub = msa_sub[np.array(selected_indices)]\n",
    "msa_sub.to_fasta('selected_msa.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_sub = ap.ProteinSequences.from_fasta('selected_msa.fasta')\n",
    "wt = msa_sub[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_positions = [182, 183, 226, 227]\n",
    "mutation_strings_base = [\n",
    "    f\"{wt[pos]}{pos}\" for pos in library_positions\n",
    "]\n",
    "def get_full_sequence(wt, aas):\n",
    "    mutation_strings = [mutation_strings_base[i]+aa for i, aa in enumerate(aas)]\n",
    "    return wt.mutate(mutation_strings, one_indexed=False)\n",
    "df['full_sequence'] = df['sequence'].apply(lambda x: get_full_sequence(wt, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and create X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=0)\n",
    "X = ap.ProteinSequences(df['full_sequence'].tolist())\n",
    "y = df['fitness'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the activity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_activity = df[df['sequence'] == 'VFVS'].iloc[0]['fitness']\n",
    "wt_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.kdeplot(y, ax=ax, bw_adjust=0.5)\n",
    "ax.vlines(wt_activity, 0, ax.get_ylim()[1], color='red', label='WT activity')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Fitness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "\n",
    "We need to:\n",
    "1. Define splits for Kfold the better resemble actual training set sizes, eg. train on 130k variants, test on remaining 20k is not reasonable. Note that plenty of work on active learning suggest we can get accuracy with fewer samples by non random selection, but acquisition functions/active learning are currently native to AIDE. It is not hard to use AIDE models in an active learning loop, but we do not have boilerplate importable code for that yet so here we are doing more standard supervised model evaluation.\n",
    "2. Define metrics that are relevant to the study, kendall's tau and top 10 recovery is a better representation of our experimental goal than spearman.\n",
    "3. Define a wrapper function to run hyperparameter optimization and test set as training size increases, over random instatiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_kfold_iterator(X, n_splits=5, shuffle=True, random_state=42, training_size=96*100, test_size='all'):\n",
    "    \"\"\"KFold iterator that uses smaller training set and larger test set\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    X: array-like\n",
    "        The data to split\n",
    "    n_splits: int\n",
    "        Number of splits\n",
    "    shuffle: bool\n",
    "        Whether to shuffle the data\n",
    "    random_state: int\n",
    "        Random state\n",
    "    training_size: int\n",
    "        Size of the training set, will be sampled from fold\n",
    "    test_size: int or 'all'\n",
    "        Size of the test set, if 'all' then all data not in training set will be used\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    np.random.seed(random_state)\n",
    "    for test_index, train_index in kf.split(X):\n",
    "        # sample to training size\n",
    "        train_index = np.random.choice(train_index, training_size, replace=False)\n",
    "        if test_size != 'all':\n",
    "            test_index = np.random.choice(test_index, test_size, replace=False)\n",
    "        yield train_index, test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_in_plate(y_true, y_pred):\n",
    "    \"\"\"Calculate top 10 recovery in a single plate, eg. how many of the top 10 true values are in the top 96 predicted values\"\"\"\n",
    "    # now get top 10 recovery in a single plate\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    top_10_idx = list(np.argsort(y_true)[-10:])\n",
    "    top_100_predicted_idx = list(np.argsort(y_pred)[-96:])\n",
    "    top_10_recovery = sum([1 for idx in top_10_idx if idx in top_100_predicted_idx]) / 10\n",
    "    return top_10_recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'kendall_tau': make_scorer(lambda y_true, y_pred: kendalltau(y_true, y_pred)[0]),\n",
    "    'top10_in_plate': make_scorer(top10_in_plate),\n",
    "    'spearman': make_scorer(lambda y_true, y_pred: spearmanr(y_true, y_pred)[0]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_1(model, X, y, training_size=96*10, random_state=42):\n",
    "    \"\"\"Cross validate a model with a single training size.\n",
    "    \"\"\"\n",
    "    return cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=modified_kfold_iterator(X, training_size=training_size, random_state=random_state),\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "def cv_increasing_data_size(model, X, y, training_sizes=[24*2, 24*3, 24*4, 24*5, 24*6, 24*7, 24*8, 24*9, 24*10, 24*12, 24*16, 24*20, 24*24, 24*100], n_repeats=20):\n",
    "    \"\"\"Cross validate a model with increasing training sizes, over a number of repeats.\"\"\"\n",
    "    results = []\n",
    "    for repeat in range(n_repeats):\n",
    "        for training_size in training_sizes:\n",
    "            results.append(cv_1(model, X, y, training_size=training_size, random_state=repeat))\n",
    "            results[-1]['training_size'] = training_size\n",
    "    return results  \n",
    "\n",
    "def hyperopt_and_scoring(model, X, y, param_distributions, ho_training_size=96*10, eval_training_sizes=[24*2, 24*3, 24*4, 24*5, 24*6, 24*7, 24*8, 24*9, 24*10, 24*12, 24*16, 24*20, 24*24, 24*100], n_repeats=20):\n",
    "    \"\"\"Hyperparameter optimization and scoring over increasing training sizes in one call.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model: sklearn estimator\n",
    "        The model to optimize and score\n",
    "    X: array-like\n",
    "    y: array-like\n",
    "    param_distributions: dict\n",
    "        The hyperparameter distributions to search over, to be fed to RandomizedSearchCV\n",
    "    ho_training_size: int\n",
    "        The size of the training set for hyperparameter optimization\n",
    "    eval_training_sizes: list of ints\n",
    "        The training sizes to evaluate the model on\n",
    "    n_repeats: int\n",
    "        Number of repeats to average over\n",
    "    \"\"\"\n",
    "    searcher = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=100,\n",
    "        n_jobs=1,\n",
    "        cv=modified_kfold_iterator(X, training_size=ho_training_size),\n",
    "        scoring=scoring,\n",
    "        refit='kendall_tau',\n",
    "    )\n",
    "    searcher.fit(X, y)\n",
    "    best_params = searcher.best_params_\n",
    "    scores = cv_increasing_data_size(model.set_params(**best_params), X, y, training_sizes=eval_training_sizes, n_repeats=n_repeats)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self supervised predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ESM2 with wildtype marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2liklihood = ap.ESM2LikelihoodWrapper(\n",
    "    wt=wt,\n",
    "    marginal_method='wildtype_marginal',\n",
    "    device='mps',\n",
    "    metadata_folder=os.path.join('.', 'data', 'epistatic', 'esm2likelihood'),\n",
    "    model_checkpoint='esm2_t33_650M_UR50D',\n",
    "    use_cache=True,\n",
    ")\n",
    "esm2liklihood.fit([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_none_none_scores = cv_1(esm2liklihood, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(esm_none_none_scores, 'esm_none_none_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 EVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc = ap.EVMutationWrapper(\n",
    "    wt=wt,\n",
    "    metadata_folder=os.path.join('.', 'data', 'epistatic', 'evmutation'),\n",
    ")\n",
    "evc.fit(msa_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc_none_none_scores = cv_1(evc, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(evc_none_none_scores, 'evc_none_none_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 MSATransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_likelihood = ap.MSATransformerLikelihoodWrapper(\n",
    "    wt=wt,\n",
    "    metadata_folder=os.path.join('.', 'data', 'epistatic', 'msalikelihood'),\n",
    "    device='mps',\n",
    "    marginal_method='wildtype_marginal',\n",
    ")\n",
    "msa_likelihood.fit(msa_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_none_none_scores = cv_1(msa_likelihood, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(msa_none_none_scores, 'msa_none_none_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 downstream pipelines\n",
    "\n",
    "We can specifiy embedders with AIDE into a scikitlearn pipeline, but to avoid recomputation of embeddings we will precompute them and pass them into specified pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_nopca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', TransformedTargetRegressor(\n",
    "        regressor=Ridge(\n",
    "            alpha=1.0,\n",
    "        )))])\n",
    "linear_pca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.9)),\n",
    "    ('model', TransformedTargetRegressor(\n",
    "        regressor=Ridge(\n",
    "            alpha=1.0,\n",
    "        )))])\n",
    "linear_param_space = {\n",
    "    'model__regressor__alpha': loguniform(1e-3, 1e3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_nopca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', TransformedTargetRegressor(\n",
    "        regressor=MLPRegressor(\n",
    "            hidden_layer_sizes=(20, 20),\n",
    "            max_iter=1000,\n",
    "            early_stopping=True\n",
    "        )))])\n",
    "mlp_pca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.9)),\n",
    "    ('model', TransformedTargetRegressor(\n",
    "        regressor=MLPRegressor(\n",
    "            hidden_layer_sizes=(20, 20),\n",
    "            max_iter=1000,\n",
    "            early_stopping=True\n",
    "        )))])\n",
    "mlp_param_space = {\n",
    "    'model__regressor__hidden_layer_sizes': [(20,), (20, 20), (20, 20, 20), (20, 20, 20, 20)],\n",
    "    'model__regressor__alpha': loguniform(1e-6, 1e-1),\n",
    "    'model__regressor__activation': ['relu', 'tanh'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 OHE linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = ap.OneHotProteinEmbedding(\n",
    "    positions=library_positions,\n",
    "    metadata_folder=os.path.join('.', 'data', 'epistatic', 'onehot'))\n",
    "X_ohe = ohe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_ohe_linear_scores = hyperopt_and_scoring(linear_nopca, X_ohe, y, linear_param_space)\n",
    "joblib.dump(none_ohe_linear_scores, 'none_ohe_linear_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 OHE MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_ohe_mlp_scores = hyperopt_and_scoring(mlp_nopca, X_ohe, y, mlp_param_space)\n",
    "joblib.dump(none_ohe_mlp_scores, 'none_ohe_mlp_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ESM2 embeddings, full sequence, linear model\n",
    "\n",
    "Include scaling and PCA to reduce dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2_embedder = ap.ESM2Embedding(\n",
    "    pool='mean',\n",
    "    metadata_folder=os.path.join('.', 'data', 'epistatic', 'esm2emb_full'),\n",
    "    device='mps',\n",
    "    model_checkpoint='esm2_t12_35M_UR50D',\n",
    "    use_cache=True,\n",
    "    batch_size=180\n",
    ")\n",
    "esm2_embedder.fit([])\n",
    "X_esm = esm2_embedder.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_esmfull_linear_scores = hyperopt_and_scoring(linear_pca, X_esm, y, linear_param_space)\n",
    "joblib.dump(none_esmfull_linear_scores, 'none_esmfull_linear_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ESM2 embeddings, full sequence, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_esmfull_mlp_scores = hyperopt_and_scoring(mlp_pca, X_esm, y, mlp_param_space)\n",
    "joblib.dump(none_esmfull_mlp_scores, 'none_esmfull_mlp_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 ESM2 embeddings, only changing residues, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2_embedder_only4 = ap.ESM2Embedding(\n",
    "    pool='mean',\n",
    "    metadata_folder=os.path.join('.', 'data', 'epistatic', 'esm2emb_design_space'),\n",
    "    device='mps',\n",
    "    model_checkpoint='esm2_t12_35M_UR50D',\n",
    "    use_cache=True,\n",
    "    batch_size=180,\n",
    "    positions=library_positions\n",
    ")\n",
    "esm2_embedder_only4.fit([])\n",
    "X_esm_only4 = esm2_embedder_only4.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_esm4sites_linear_scores = hyperopt_and_scoring(linear_pca, X_esm_only4, y, linear_param_space)\n",
    "joblib.dump(none_esm4sites_linear_scores, 'none_esm4sites_linear_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 ESM2 embeddings, only changing residues, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_esm4sites_mlp_scores = hyperopt_and_scoring(mlp_pca, X_esm_only4, y, mlp_param_space)\n",
    "joblib.dump(none_esm4sites_mlp_scores, 'none_esm4sites_mlp_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combined models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 EVC plus one hot linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_evc = evc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.concatenate([X_ohe, X_evc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc_ohe_linear_scores = hyperopt_and_scoring(linear_nopca, X_, y, linear_param_space)\n",
    "joblib.dump(evc_ohe_linear_scores, 'evc_ohe_linear_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 EVC plus one hot MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc_ohe_mlp_scores = hyperopt_and_scoring(mlp_nopca, X_, y, mlp_param_space)\n",
    "joblib.dump(evc_ohe_mlp_scores, 'evc_ohe_mlp_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 EVC plus ESM2 linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.concatenate([X_esm, X_evc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc_esmfull_linear_scores = hyperopt_and_scoring(linear_pca, X_, y, linear_param_space)\n",
    "joblib.dump(evc_esmfull_linear_scores, 'evc_esmfull_linear_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 EVC plus ESM2 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc_esmfull_mlp_scores = hyperopt_and_scoring(mlp_pca, X_, y, mlp_param_space)\n",
    "joblib.dump(evc_esmfull_mlp_scores, 'evc_esmfull_mlp_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 EVC plus ESM2 only 4 linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.concatenate([X_esm_only4, X_evc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc_esm4sites_linear_scores = hyperopt_and_scoring(linear_pca, X_, y, linear_param_space)\n",
    "joblib.dump(evc_esm4sites_linear_scores, 'evc_esm4sites_linear_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 EVC plus ESM2 only 4 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc_esm4sites_mlp_scores = hyperopt_and_scoring(mlp_pca, X_, y, mlp_param_space)\n",
    "joblib.dump(evc_esm4sites_mlp_scores, 'evc_esm4sites_mlp_scores.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_scores = {\n",
    "    'evc_none_none': joblib.load('evc_none_none_scores.joblib'),\n",
    "    'esm_none_none': joblib.load('esm_none_none_scores.joblib'),\n",
    "    'msa_none_none': joblib.load('msa_none_none_scores.joblib'),\n",
    "}\n",
    "supervised_scores = {\n",
    "    'none_ohe_linear': joblib.load('none_ohe_linear_scores.joblib'),\n",
    "    'none_ohe_mlp': joblib.load('none_ohe_mlp_scores.joblib'),\n",
    "    'none_esmfull_linear': joblib.load('none_esmfull_linear_scores.joblib'),\n",
    "    'none_esmfull_mlp': joblib.load('none_esmfull_mlp_scores.joblib'),\n",
    "    'none_esm4sites_linear': joblib.load('none_esm4sites_linear_scores.joblib'),\n",
    "    'none_esm4sites_mlp': joblib.load('none_esm4sites_mlp_scores.joblib'),\n",
    "    'evc_ohe_linear': joblib.load('evc_ohe_linear_scores.joblib'),\n",
    "    'evc_ohe_mlp': joblib.load('evc_ohe_mlp_scores.joblib'),\n",
    "    'evc_esmfull_linear': joblib.load('evc_esmfull_linear_scores.joblib'),\n",
    "    'evc_esmfull_mlp': joblib.load('evc_esmfull_mlp_scores.joblib'),\n",
    "    'evc_esm4sites_linear': joblib.load('evc_esm4sites_linear_scores.joblib'),\n",
    "    'evc_esm4sites_mlp': joblib.load('evc_esm4sites_mlp_scores.joblib'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for model, score_list in supervised_scores.items():\n",
    "    for score_dict in score_list:\n",
    "        training_size = score_dict['training_size']\n",
    "        for metric in ['kendall_tau', 'spearman', 'top10_in_plate']:\n",
    "            for point in score_dict[f'test_{metric}']:\n",
    "                data.append({\n",
    "                    'Model': model,\n",
    "                    'Metric': metric,\n",
    "                    'Score': point,\n",
    "                    'Training Size': training_size\n",
    "                })\n",
    "\n",
    "supervised_df = pd.DataFrame(data)\n",
    "supervised_df[['zs', 'embedding', 'topmodel']] = supervised_df['Model'].str.split('_', expand=True)\n",
    "data = []\n",
    "for model, scores in zs_scores.items():\n",
    "    for metric in ['kendall_tau', 'spearman', 'top10_in_plate']:\n",
    "        for point in scores[f'test_{metric}']:\n",
    "            data.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric,\n",
    "                'Score': point,\n",
    "                'Training Size': 0.0\n",
    "            })\n",
    "zs_df = pd.DataFrame(data)\n",
    "zs_df[['zs', 'embedding', 'topmodel']] = zs_df['Model'].str.split('_', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determined by embedding type\n",
    "color_map = {\n",
    "    'none': 'grey',\n",
    "    'ohe': 'tab:blue',\n",
    "    'esmfull': 'tab:orange',\n",
    "    'esm4sites': 'tab:green',\n",
    "}\n",
    "# determined by ZS method\n",
    "linestyle_map = {\n",
    "    'none': ':',\n",
    "    'evc': '-',\n",
    "    'esm': '--',\n",
    "    'msa': '-.',\n",
    "}\n",
    "# determined by top model\n",
    "marker_map = {\n",
    "    'none': 'o',\n",
    "    'linear': 's',\n",
    "    'mlp': 'd',\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of ZS vs supervised vs augmented linear, like original paper\n",
    "metric = 'kendall_tau'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for model, df in supervised_df.groupby('Model'):\n",
    "    df_ = df[df['Metric'] == metric]\n",
    "    color, linestyle, marker = color_map[model.split('_')[1]], linestyle_map[model.split('_')[0]], marker_map[model.split('_')[2]]\n",
    "    if model.split('_')[2] =='mlp':\n",
    "        continue\n",
    "    label = model\n",
    "    if model.split('_')[0] == 'none':\n",
    "        label += ' (Supervised)'\n",
    "    else:\n",
    "        label += ' (Augmented)'\n",
    "    sns.lineplot(x='Training Size', y='Score', data=df_, ax=ax,\n",
    "                    label=label, color=color, linestyle=linestyle, marker=marker)\n",
    "\n",
    "for model, df in zs_df.groupby('Model'):\n",
    "    df_ = df[df['Metric'] == metric]\n",
    "    color, linestyle, marker = color_map[model.split('_')[1]], linestyle_map[model.split('_')[0]], marker_map[model.split('_')[2]]\n",
    "    label = model + ' (Zero-shot)'\n",
    "    xmin = ax.get_xlim()[0]\n",
    "    xmax = ax.get_xlim()[1]\n",
    "    mean = df_.mean()['Score']\n",
    "    ax.hlines(mean, xmin, xmax, color=color, linestyle=linestyle, label=label)\n",
    "\n",
    "ax.set_xlabel('Training Size')\n",
    "ax.set_ylabel(\"Kendall's Tau\")\n",
    "plt.legend()\n",
    "plt.savefig('zs_vs_supervised_vs_augmented.png', bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compare linear to nonlinear models\n",
    "metric = 'kendall_tau'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for model, df in supervised_df.groupby('Model'):\n",
    "    df_ = df[df['Metric'] == metric]\n",
    "    color, linestyle, marker = color_map[model.split('_')[1]], linestyle_map[model.split('_')[0]], marker_map[model.split('_')[2]]\n",
    "    if model.split('_')[0] != 'none':\n",
    "        continue\n",
    "    label = model\n",
    "    if model.split('_')[0] == 'none':\n",
    "        label += ' (Supervised)'\n",
    "    else:\n",
    "        label += ' (Augmented)'\n",
    "    sns.lineplot(x='Training Size', y='Score', data=df_, ax=ax,\n",
    "                    label=label, color=color, linestyle=linestyle, marker=marker)\n",
    "\n",
    "ax.set_xlabel('Training Size')\n",
    "ax.set_ylabel(\"Kendall's Tau\")\n",
    "plt.legend()\n",
    "plt.savefig('nonlinear_vs_linear_kendall.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compare linear to nonlinear models\n",
    "metric = 'top10_in_plate'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for model, df in supervised_df.groupby('Model'):\n",
    "    df_ = df[df['Metric'] == metric]\n",
    "    color, linestyle, marker = color_map[model.split('_')[1]], linestyle_map[model.split('_')[0]], marker_map[model.split('_')[2]]\n",
    "    if model.split('_')[0] != 'none':\n",
    "        continue\n",
    "    label = model\n",
    "    if model.split('_')[0] == 'none':\n",
    "        label += ' (Supervised)'\n",
    "    else:\n",
    "        label += ' (Augmented)'\n",
    "    sns.lineplot(x='Training Size', y='Score', data=df_, ax=ax,\n",
    "                    label=label, color=color, linestyle=linestyle, marker=marker)\n",
    "\n",
    "ax.set_xlabel('Training Size')\n",
    "ax.set_ylabel(\"Top10 Recovery, 1 96w-plate\")\n",
    "plt.legend()\n",
    "plt.savefig('nonlinear_vs_linear_top10.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
